import pandas as pd
import string
import re
import numpy as np
from underthesea import word_tokenize, text_normalize
import unicodedata

# Biá»ƒu thá»©c chÃ­nh quy Ä‘á»ƒ nháº­n diá»‡n vÃ  giá»¯ láº¡i cÃ¡c emoji
emoji_pattern = re.compile(
    u"["
    u"\U0001F600-\U0001F64F"  # Máº·t cÆ°á»i, biá»ƒu cáº£m
    u"\U0001F44D"  # Like (ğŸ‘)
    u"\U0001F44E"  # Dislike (ğŸ‘)
    u"\U0001F4AA"  # Biá»ƒu tÆ°á»£ng cÆ¡ báº¯p (ğŸ’ª)
    u"\U0001F91D"  # Biá»ƒu tÆ°á»£ng báº¯t tay (ğŸ¤)
    u"\U0001F44C"  # Biá»ƒu tÆ°á»£ng tay váº«y (ğŸ‘Œ)
    u"\U00002705"  # Biá»ƒu tÆ°á»£ng tick (âœ…)
    u"\U0000274C"  # Biá»ƒu tÆ°á»£ng cross (âŒ)
    u"\U00002764"
    u"]+", flags=re.UNICODE)

# HÃ m Ä‘á»ƒ loáº¡i bá» cÃ¡c tá»« khÃ´ng pháº£i tiáº¿ng Anh, tiáº¿ng Viá»‡t (bao gá»“m tiáº¿ng Trung) nhÆ°ng giá»¯ láº¡i emoji
# HÃ m Ä‘á»ƒ giá»¯ láº¡i emoji Ä‘Ãºng vá»‹ trÃ­
emoji_pattern_2 = re.compile(
    r"(âœ…|âŒ|ğŸ‘Œ|ğŸ‘|ğŸ˜€|ğŸ˜|ğŸ˜‚|ğŸ˜ƒ|ğŸ˜„|ğŸ˜…|ğŸ˜†|ğŸ˜‡|ğŸ˜‰|ğŸ˜Š|ğŸ˜‹|ğŸ˜|ğŸ˜Œ|ğŸ˜|ğŸ˜—|ğŸ˜˜|ğŸ˜™|ğŸ˜š|ğŸ˜›|ğŸ˜œ|ğŸ˜|ğŸ˜|ğŸ˜Ÿ|ğŸ˜ |ğŸ˜¡|ğŸ˜¢|ğŸ˜£|ğŸ˜¤|ğŸ˜¥|ğŸ˜¦|ğŸ˜©|ğŸ˜«|ğŸ˜¬|ğŸ˜­|ğŸ˜®|ğŸ˜±|ğŸ˜²|ğŸ˜³|ğŸ˜´|ğŸ˜µ|ğŸ˜¶|ğŸ˜¸|ğŸ˜¹|ğŸ˜»|ğŸ˜¼|ğŸ™|ğŸ™‚|ğŸ™ƒ|ğŸ™„|ğŸ™…|ğŸ™†|ğŸ™ˆ|ğŸ¤)(\1+)",
    re.UNICODE)

emoji_pattern_3 = re.compile(
    u"["
    u"\U0001F600-\U0001F64F"  # Máº·t cÆ°á»i, biá»ƒu cáº£m
    u"\U0001F300-\U0001F5FF"  # Biá»ƒu tÆ°á»£ng thiÃªn vÄƒn, Ä‘á»“ váº­t
    u"\U0001F680-\U0001F6FF"  # PhÆ°Æ¡ng tiá»‡n giao thÃ´ng
    u"\U0001F1E0-\U0001F1FF"  # Quá»‘c ká»³
    u"\U00002702-\U000027B0"  # CÃ¡c biá»ƒu tÆ°á»£ng khÃ¡c
    u"\U000024C2-\U0001F251"  # CÃ¡c biá»ƒu tÆ°á»£ng khÃ¡c
    u"\U0001f926-\U0001f937"  # CÃ¡c biá»ƒu tÆ°á»£ng ngÆ°á»i
    u'\U00010000-\U0010ffff'  # CÃ¡c emoji bá»• sung
    u"\u200d"  # Dáº¥u ná»‘i cho emoji phá»©c há»£p
    u"\u2640-\u2642"  # Biá»ƒu tÆ°á»£ng giá»›i tÃ­nh
    u"\u2600-\u2B55"  # Biá»ƒu tÆ°á»£ng máº·t trá»i, sao, giao thÃ´ng
    u"\u23cf"  # Dá»«ng
    u"\u23e9"  # Biá»ƒu tÆ°á»£ng phÃ¡t video
    u"\u231a"  # Äá»“ng há»“
    u"\u3030"  # Biá»ƒu tÆ°á»£ng ná»‘t nháº¡c
    u"\ufe0f"  # Biá»ƒu tÆ°á»£ng cáº£m xÃºc vá»›i dáº¡ng hÃ¬nh áº£nh
    u"\U0001F44D"  # Like (ğŸ‘)
    u"\U0001F44E"  # Dislike (ğŸ‘)
    u"\U0001F4AA"  # Biá»ƒu tÆ°á»£ng cÆ¡ báº¯p (ğŸ’ª)
    u"\U0001F91D"  # Biá»ƒu tÆ°á»£ng báº¯t tay (ğŸ¤)
    u"\U0001F44C"  # Biá»ƒu tÆ°á»£ng tay váº«y (ğŸ‘Œ)
    u"\U00002705"  # Biá»ƒu tÆ°á»£ng tick (âœ…)
    u"\U0000274C"  # Biá»ƒu tÆ°á»£ng cross (âŒ)
    u"]+", flags=re.UNICODE)


# HÃ m lÃ m sáº¡ch vÄƒn báº£n vÃ  loáº¡i bá» cÃ¡c emoji láº·p láº¡i
def clean_text_and_remove_duplicates(text):
    # Sá»­ dá»¥ng regex Ä‘á»ƒ thay tháº¿ cÃ¡c emoji láº·p láº¡i nhiá»u láº§n thÃ nh chá»‰ má»™t láº§n
    cleaned_text = re.sub(emoji_pattern_2, r'\1', text)
    return cleaned_text


def clean_text_and_preserve_emojis(text):
    text = clean_text_and_remove_duplicates(text)
    # TÃ¬m cÃ¡c emoji vÃ  lÆ°u vá»‹ trÃ­ cá»§a chÃºng
    emoji_positions = [(match.start(), match.group()) for match in emoji_pattern.finditer(text)]

    # Loáº¡i bá» emoji khá»i vÄƒn báº£n
    text_without_emoji = emoji_pattern.sub('', text)

    # LÃ m sáº¡ch vÄƒn báº£n khÃ´ng cÃ³ emoji
    cleaned_text = re.sub(r'[^a-zA-ZÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄÄ‘\s]+', ' ',
                          text_without_emoji)

    # ChÃ¨n láº¡i emoji vÃ o vá»‹ trÃ­ ban Ä‘áº§u
    result = list(cleaned_text)
    for pos, emoji in emoji_positions:
        result.insert(pos, emoji)

    return ''.join(result)


abb_dict = pd.read_excel('trien_khai_model/preprocessing/normal.xlsx').set_index('abbreviation')['meaning'].to_dict()
character2emoji = pd.read_excel('trien_khai_model/preprocessing/character2emoji.xlsx').set_index('Character')['Emoji'].to_dict()
emoji2word = pd.read_excel('trien_khai_model/preprocessing/emoji2word.xlsx').set_index('Emoji')['Meaning'].to_dict()

with open("trien_khai_model/preprocessing/vn_stopwords.txt", encoding='utf-8') as file:
    stopwords = [
        re.escape(unicodedata.normalize('NFC', line.strip().lower()))
        for line in file.read().splitlines() if line.strip()
    ]


def remove_meaningless_words(text, delete_meaningless_words):
    words = text.split()

    cleaned_words = [word for word in words if word.lower() not in delete_meaningless_words]

    return " ".join(cleaned_words)


def abbreviation_normal(text, abb_dict):
    words = text.lower().split()
    return ' '.join(abb_dict.get(word, word) for word in words).strip()


def convert_character2emoji(text, character2emoji):
    for char, emoji in character2emoji.items():
        text = text.replace(char, f" {emoji} ")
    return text.strip()


def convert_emoji2word(text, emoji2word):
    for emoji, word in emoji2word.items():
        text = text.replace(emoji, f" {word} ")
    return text.strip()


stopwords_pattern = r'\b(?:' + '|'.join(stopwords) + r')\b'


def remove_stopword(text):
    # Loáº¡i bá» stopwords báº±ng regex
    return re.sub(stopwords_pattern, '', text, flags=re.IGNORECASE).strip()


with open("trien_khai_model/preprocessing/merged_file_no_duplicates.txt", encoding='utf-8') as file:
    delete_meaningless_words = [unicodedata.normalize('NFC', line.lower()) for line in file.read().splitlines()]

dict_map = {
    "Ã²a": "oÃ ",
    "Ã³a": "oÃ¡",
    "á»a": "oáº£",
    "Ãµa": "oÃ£",
    "á»a": "oáº¡",
    "Ã²e": "oÃ¨",
    "Ã³e": "oÃ©",
    "á»e": "oáº»",
    "Ãµe": "oáº½",
    "á»e": "oáº¹",
    "Ã¹y": "uá»³",
    "Ãºy": "uÃ½",
    "á»§y": "uá»·",
    "Å©y": "uá»¹",
    "á»¥y": "uá»µ",
}


def replace_all(text):
    for i, j in dict_map.items():
        text = text.replace(i, j)
    return text


def clean_text(text, remove_emoji=False, convert_e2w=False, normalize=False, tokenize=False, remove_stopwords=False):
    if re.search(r'https?://\S+|www\.\S+', text):
        return None

    text = text.lower()
    text = unicodedata.normalize('NFC', text)
    text = clean_text_and_preserve_emojis(text)

    # XÃ³a kÃ½ tá»± xuá»‘ng dÃ²ng
    text = re.sub(r'\r\n', '', text)

    # Loáº¡i bá» cÃ¡c kÃ½ tá»± ğŸ™Œ vÃ  ğŸ™
    text = re.sub(r'[ğŸ™Œ]+', '', text)
    text = re.sub(r'[ğŸ™]+', '', text)
    text = re.sub(r'\r\n', '', text)

    # Loáº¡i bá» cÃ¡c tá»« vÃ´ nghÄ©a
    text = remove_meaningless_words(text, delete_meaningless_words)

    # Thay tháº¿ emoji
    text = re.sub(emoji_pattern_2, r'\1', text)
    text = abbreviation_normal(text, abb_dict)
    text = replace_all(text)

    # Loáº¡i bá» emoji náº¿u cáº§n
    if remove_emoji:
        text = re.sub(emoji_pattern, " ", text)

    # Xá»­ lÃ½ cÃ¡c tá»« láº·p láº¡i
    text = re.sub(r'([a-z]+?)\1+', r'\1', text)

    text = re.sub(r'["\']', '', text)

    # Xá»­ lÃ½ dáº¥u cÃ¢u
    text = re.sub(r"(\w)\s*([" + string.punctuation + "])\s*(\w)", r"\1 \2 \3", text)
    text = re.sub(r"(\w)\s*([" + string.punctuation + "])", r"\1 \2", text)
    text = re.sub(f"([{string.punctuation}])([{string.punctuation}])+", r"\1", text)

    # Loáº¡i bá» cÃ¡c khoáº£ng tráº¯ng thá»«a
    text = text.strip().strip(string.punctuation)
    text = re.sub(r"\s+", " ", text)

    # Chuyá»ƒn emoji thÃ nh tá»« náº¿u yÃªu cáº§u
    if convert_e2w:
        text = convert_character2emoji(text, character2emoji)
        text = convert_emoji2word(text, emoji2word)

    if normalize:
        text = text_normalize(text)

    # Token hÃ³a vÄƒn báº£n náº¿u yÃªu cáº§u
    if tokenize:
        text = word_tokenize(text, format="text")
    # Loáº¡i bá» stopwords náº¿u yÃªu cáº§u
    if remove_stopwords:
        text = remove_stopword(text)
    return text


def check_emoji(text):
    # TÃ¬m táº¥t cáº£ emoji trong vÄƒn báº£n
    emojis = emoji_pattern.findall(' '.join(text))

    return emojis, np.unique(emojis)